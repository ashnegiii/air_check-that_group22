69c678b6ffcd:/subtask_4b# cat outpyt.log
nohup: ignoring input
INFO:__main__:Loading full datasets...
INFO:__main__:Loaded 7718 papers, 12853 train queries, 1400 dev queries
INFO:__main__:Initializing ranker...
INFO:__main__:Using device: cuda
/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
INFO:__main__:Loaded tokenizer: bert-base-uncased
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:__main__:Loaded cached model: bert-base-uncased
INFO:__main__:Ranker initialized successfully!
INFO:__main__:Preparing TF-IDF retrieval...
INFO:__main__:Preparing TF-IDF model...
INFO:__main__:Cleaned collection: 7718 papers (from 7718)
INFO:__main__:TF-IDF model prepared!
INFO:__main__:Starting full training...
INFO:__main__:Starting training...
INFO:__main__:Preparing training data...
INFO:__main__:Processing 12853 queries (from 12853)
100%|██████████| 12853/12853 [03:40<00:00, 58.31it/s]
INFO:__main__:Prepared 77118 training samples
INFO:__main__:Positive samples: 12853.0
INFO:__main__:Negative samples: 64265.0
INFO:__main__:Preparing training data...
INFO:__main__:Processing 1400 queries (from 1400)
100%|██████████| 1400/1400 [00:24<00:00, 56.51it/s]
INFO:__main__:Prepared 5600 training samples
INFO:__main__:Positive samples: 1400.0
INFO:__main__:Negative samples: 4200.0
INFO:__main__:Dataset cleaned: 77118 samples remaining
INFO:__main__:Dataset cleaned: 5600 samples remaining
INFO:__main__:
Epoch 1/3
Training: 100%|██████████| 9639/9639 [30:34<00:00,  5.25it/s, loss=0.2354, acc=0.8731]
Validation: 100%|██████████| 700/700 [00:44<00:00, 15.64it/s]
INFO:__main__:Train Loss: 0.3771, Train Acc: 0.8731
INFO:__main__:Val Loss: 0.4252, Val Acc: 0.8436
INFO:__main__:Saved best model!
INFO:__main__:
Epoch 2/3
Training: 100%|██████████| 9639/9639 [30:41<00:00,  5.23it/s, loss=0.4725, acc=0.8958]
Validation: 100%|██████████| 700/700 [00:44<00:00, 15.60it/s]
INFO:__main__:Train Loss: 0.3370, Train Acc: 0.8957
INFO:__main__:Val Loss: 0.4261, Val Acc: 0.8430
INFO:__main__:
Epoch 3/3
Training: 100%|██████████| 9639/9639 [30:39<00:00,  5.24it/s, loss=0.0177, acc=0.9043]
Validation: 100%|██████████| 700/700 [00:44<00:00, 15.60it/s]
INFO:__main__:Train Loss: 0.3151, Train Acc: 0.9042
INFO:__main__:Val Loss: 0.4927, Val Acc: 0.8439
INFO:__main__:Best model loaded!
INFO:__main__:Evaluating on full dev set...
INFO:__main__:Evaluating ranker...
100%|██████████| 1400/1400 [21:29<00:00,  1.09it/s]
INFO:__main__:MRR@5: 0.4957
INFO:__main__:Final Dev MRR@5: 0.4957